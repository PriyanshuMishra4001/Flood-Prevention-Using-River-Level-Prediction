{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries here \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import HuberRegressor, LassoCV, ElasticNetCV, LassoLarsCV\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LassoLarsIC, LarsCV, Lars, RANSACRegressor, ElasticNet, Lasso, OrthogonalMatchingPursuitCV, PassiveAggressiveRegressor, OrthogonalMatchingPursuit, LassoLars\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import RidgeCV, BayesianRidge, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function deals with loading the data and doing some minor operations\n",
    "def load_and_preprocess_data(file_path, selected_columns):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data[selected_columns]\n",
    "    data = data.dropna()\n",
    "    pressure_mean = data['Pressure'].mean()  # Calculate the mean of the Pressure column\n",
    "    print(\"Mean Pressure \", pressure_mean)\n",
    "    print(data[data['Pressure'] == 0].count())\n",
    "    data.loc[data['Pressure'] == 0, 'Pressure'] = pressure_mean  # Replace all the rows with the mean value if the pressure is recorded as zero\n",
    "    print(data[data['Pressure'] == 0].count())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions splits the data into feature and target\n",
    "def split_features_target(data, target_column):\n",
    "    X = data.drop(target_column, axis=1)\n",
    "    y = data[target_column]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function splits the data in training and testing\n",
    "def split_train_test(X, y, test_size=0.3, random_state=420):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to scale the data\n",
    "def scale_data(X_train, X_test, y_train, y_test):\n",
    "    feature_scaler = StandardScaler()\n",
    "    X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, feature_scaler, target_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['Windspeed', 'Humidity', 'Temperature', 'Dewpoint', 'Pressure', 'Reading', 'Wind direction', 'Level']\n",
    "\n",
    "merged_df = load_and_preprocess_data('./Datasets/2021-Kippure.csv', selected_columns) # Calling the function to create a dataframe with the above mentioned columns\n",
    "    \n",
    "X, y = split_features_target(merged_df, 'Level') # Calling the function to split the data into features and target\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_train_test(X, y) # Splitting the data into training and testing\n",
    "\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, feature_scaler, target_scaler = scale_data(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a dictionary containing the different models.\n",
    "models = {\n",
    "    \"Extra Trees Regressor\": ExtraTreesRegressor(),\n",
    "    \"AdaBoost Regressor\": AdaBoostRegressor(),\n",
    "    \"Gradient Boosting Regressor\": GradientBoostingRegressor(),\n",
    "    \"HistGradient Boosting Regressor\": HistGradientBoostingRegressor(),\n",
    "    \"K Neighbors Regressor\": KNeighborsRegressor(),\n",
    "    \"Bagging Regressor\": BaggingRegressor(),\n",
    "    \"Huber Regressor\": HuberRegressor(),\n",
    "    \"Transformed Target Regressor\": TransformedTargetRegressor(regressor=HuberRegressor(), transformer=StandardScaler()),\n",
    "    \"LassoCV\": LassoCV(),\n",
    "    \"ElasticNetCV\": ElasticNetCV(),\n",
    "    \"LassoLarsCV\": LassoLarsCV(),\n",
    "    \"LassoLarsIC\": LassoLarsIC(),\n",
    "    \"LarsCV\": LarsCV(),\n",
    "    \"Lars\": Lars(),\n",
    "    \"RANSAC Regressor\": RANSACRegressor(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"Orthogonal Matching Pursuit CV\": OrthogonalMatchingPursuitCV(),\n",
    "    \"Orthogonal Matching Pursuit\": OrthogonalMatchingPursuit(),\n",
    "    \"Dummy Regressor\": DummyRegressor(),\n",
    "    \"Lasso Lars\": LassoLars(),\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred_scaled = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform to get original scale predictions\n",
    "    y_pred = target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test_original = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Evaluation metrics on the scaled dataset\n",
    "    r2 = r2_score(y_test_scaled, y_pred_scaled)\n",
    "    mse = mean_squared_error(y_test_scaled, y_pred_scaled)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse, mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "c = 0\n",
    "for name, model in models.items():\n",
    "    c+= 1\n",
    "    rmse, mse, r2 = evaluate_model(model, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "    results[name] = {\"RMSE\": rmse, \"MSE\": mse, \"R2\": r2}\n",
    "    print(c)\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
